#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë§¤ì£¼ Global Scouter ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ìëŠ” Global_Scouter í´ë”ë§Œ ìµœì‹  ë²„ì „ìœ¼ë¡œ êµì²´í•˜ë©´ ë¨
"""

import pandas as pd
import json
import sys
import io
from pathlib import Path
from datetime import datetime
import shutil

# UTF-8 ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

class GlobalScouterDataProcessor:
    """Global Scouter ëª¨ë“  CSV íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ê³  í†µí•©"""

    def __init__(self, source_dir, output_dir):
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        self.backup_dir = output_dir / 'backups'
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(exist_ok=True, parents=True)

        # CSV íŒŒì¼ ë¶„ë¥˜
        self.csv_files = {
            # í•µì‹¬ ë°ì´í„° (Main)
            'main': {
                'M_Company.csv': 'ëª¨ë©˜í…€ ê¸°ì—… ë°ì´í„° (6000+ ê¸°ì—…)',
                'A_Company.csv': 'ë¶„ì„ ê¸°ì—… ë¦¬ìŠ¤íŠ¸',
            },

            # ê¸°ìˆ  ì§€í‘œ (Technical)
            'technical': {
                'T_Chart.csv': 'ì°¨íŠ¸ ë°ì´í„°',
                'T_Rank.csv': 'ìˆœìœ„ ë°ì´í„°',
                'T_Growth_H.csv': 'ì„±ì¥ë¥  (History)',
                'T_Growth_C.csv': 'ì„±ì¥ë¥  (Current)',
                'T_EPS_H.csv': 'EPS (History)',
                'T_EPS_C.csv': 'EPS (Current)',
                'T_CFO.csv': 'í˜„ê¸ˆíë¦„',
                'T_Correlation.csv': 'ìƒê´€ê´€ê³„',
                'T_Chk.csv': 'ì²´í¬ë¦¬ìŠ¤íŠ¸',
            },

            # ë¶„ì„ ë°ì´í„° (Analysis)
            'analysis': {
                'A_Compare.csv': 'ê¸°ì—… ë¹„êµ',
                'A_Contrast.csv': 'ëŒ€ì¡° ë¶„ì„',
                'A_Distribution.csv': 'ë¶„í¬ ë¶„ì„',
                'A_ETFs.csv': 'ETF ë°ì´í„°',
            },

            # ì‹œì¥ ë°ì´í„° (Market)
            'market': {
                'S_Chart.csv': 'ì°¨íŠ¸ ìŠ¤ëƒ…ìƒ·',
                'S_Mylist.csv': 'ê´€ì‹¬ ì¢…ëª©',
                'S_Valuation.csv': 'ë°¸ë¥˜ì—ì´ì…˜',
                'UP_&_Down.csv': 'ë“±ë½ ë°ì´í„°',
            },

            # ì§€í‘œ (Indicators)
            'indicators': {
                'E_Indicators.csv': 'ê²½ì œ ì§€í‘œ',
                'M_ETFs.csv': 'ëª¨ë©˜í…€ ETF',
            }
        }

    def backup_existing_data(self):
        """ê¸°ì¡´ JSON íŒŒì¼ ë°±ì—…"""
        print(f'\\n=== ê¸°ì¡´ ë°ì´í„° ë°±ì—… ì‹œì‘ ===')
        json_files = list(self.output_dir.glob('*.json'))

        if not json_files:
            print('ë°±ì—…í•  íŒŒì¼ ì—†ìŒ')
            return

        backup_folder = self.backup_dir / f'backup_{self.timestamp}'
        backup_folder.mkdir(exist_ok=True)

        for json_file in json_files:
            if json_file.name.startswith('enhanced_summary_data'):
                backup_path = backup_folder / json_file.name
                shutil.copy2(json_file, backup_path)
                print(f'ë°±ì—…: {json_file.name} -> {backup_path.name}')

        print(f'ë°±ì—… ì™„ë£Œ: {backup_folder}')

    def process_csv(self, csv_filename):
        """ê°œë³„ CSV íŒŒì¼ ì²˜ë¦¬"""
        csv_path = self.source_dir / csv_filename

        if not csv_path.exists():
            print(f'âš ï¸ íŒŒì¼ ì—†ìŒ: {csv_filename}')
            return None

        print(f'ì²˜ë¦¬ ì¤‘: {csv_filename}')

        try:
            # CSV ì½ê¸°
            df = pd.read_csv(csv_path, encoding='utf-8')

            # í—¤ë” ì •ë¦¬ (2ë²ˆì§¸ í–‰ì´ ì‹¤ì œ í—¤ë”ì¸ ê²½ìš°)
            if df.iloc[0].isna().all() or any('Company' in str(val) or 'Corp' in str(val) for val in df.iloc[1].values if pd.notna(val)):
                df.columns = df.iloc[1].values
                df = df.iloc[2:].reset_index(drop=True)

            # DataFrame to dict
            data_dict = df.to_dict(orient='records')

            # NaN/Infinity ì •ë¦¬
            clean_data = []
            for record in data_dict:
                clean_record = {}
                for key, value in record.items():
                    if pd.isna(value):
                        clean_record[key] = None
                    elif isinstance(value, float) and (value == float('inf') or value == float('-inf')):
                        clean_record[key] = None
                    else:
                        clean_record[key] = value
                clean_data.append(clean_record)

            print(f'âœ… {csv_filename}: {len(clean_data)}ê°œ ë ˆì½”ë“œ')
            return clean_data

        except Exception as e:
            print(f'âŒ {csv_filename} ì²˜ë¦¬ ì˜¤ë¥˜: {e}')
            return None

    def process_all_categories(self):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ CSV ì²˜ë¦¬"""
        print(f'\\n=== ì „ì²´ CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===')

        results = {}

        for category, files in self.csv_files.items():
            print(f'\\n[{category.upper()}]')
            category_data = {}

            for csv_file, description in files.items():
                print(f'  {description}')
                data = self.process_csv(csv_file)

                if data:
                    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
                    key = csv_file.replace('.csv', '')
                    category_data[key] = data

            results[category] = category_data

        return results

    def generate_m_company_json(self):
        """M_Company.csvë¥¼ enhanced_summary_data_full.jsonìœ¼ë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§)"""
        print(f'\\n=== M_Company.csv -> enhanced_summary_data_full.json ë³€í™˜ ===')

        csv_path = self.source_dir / 'M_Company.csv'
        output_json = self.output_dir / 'enhanced_summary_data_full.json'

        df = pd.read_csv(csv_path, encoding='utf-8')
        df.columns = df.iloc[1].values
        df = df.iloc[2:].reset_index(drop=True)

        data_dict = df.to_dict(orient='records')

        companies = []
        for record in data_dict:
            company = {}
            for key, value in record.items():
                if pd.isna(value):
                    company[key] = None
                elif isinstance(value, float) and (value == float('inf') or value == float('-inf')):
                    company[key] = None
                else:
                    company[key] = value

            # Tickerì™€ Corp ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            if 'Ticker' in company and 'Corp' in company:
                if company['Ticker'] is not None and company['Corp'] is not None:
                    company['corpName'] = company.get('Corp', '')
                    companies.append(company)

        output_data = {
            'metadata': {
                'source': 'M_Company.csv',
                'generated_at': datetime.now().isoformat(),
                'total_companies': len(companies),
                'update_timestamp': self.timestamp,
            },
            'companies': companies
        }

        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f'âœ… {output_json.name} ìƒì„±: {len(companies)}ê°œ ê¸°ì—…')
        return len(companies)

    def generate_integrated_json(self, all_data):
        """ëª¨ë“  CSV ë°ì´í„°ë¥¼ í†µí•©í•œ JSON ìƒì„±"""
        print(f'\\n=== í†µí•© JSON ìƒì„± ===')

        output_json = self.output_dir / 'global_scouter_integrated.json'

        integrated_data = {
            'metadata': {
                'source': 'Global_Scouter',
                'generated_at': datetime.now().isoformat(),
                'update_timestamp': self.timestamp,
                'categories': list(all_data.keys()),
                'total_files': sum(len(files) for files in all_data.values()),
            },
            'data': all_data
        }

        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(integrated_data, f, ensure_ascii=False, indent=2)

        print(f'âœ… {output_json.name} ìƒì„±')

        # í†µê³„ ì¶œë ¥
        print(f'\\ní†µí•© ë°ì´í„° í†µê³„:')
        for category, files in all_data.items():
            print(f'  [{category}]: {len(files)}ê°œ íŒŒì¼')
            for filename, records in files.items():
                print(f'    - {filename}: {len(records)}ê°œ ë ˆì½”ë“œ')

        return output_json

    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print(f'â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
        print(f'â•‘   Global Scouter ë§¤ì£¼ ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸          â•‘')
        print(f'â•‘   ì‹œì‘ ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}                 â•‘')
        print(f'â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')

        # 1. ê¸°ì¡´ ë°ì´í„° ë°±ì—…
        self.backup_existing_data()

        # 2. M_Company.csv â†’ enhanced_summary_data_full.json (ê¸°ì¡´ ë¡œì§)
        total_companies = self.generate_m_company_json()

        # 3. ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬
        all_data = self.process_all_categories()

        # 4. í†µí•© JSON ìƒì„±
        integrated_json = self.generate_integrated_json(all_data)

        # 5. ì™„ë£Œ ë³´ê³ 
        print(f'\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
        print(f'â•‘              ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!                    â•‘')
        print(f'â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')
        print(f'\\nğŸ“Š ê²°ê³¼:')
        print(f'  - ë©”ì¸ ë°ì´í„°: {total_companies}ê°œ ê¸°ì—…')
        print(f'  - í†µí•© íŒŒì¼: {integrated_json.name}')
        print(f'  - ë°±ì—… ìœ„ì¹˜: {self.backup_dir / f"backup_{self.timestamp}"}')
        print(f'\\nâœ… Stock Analyzerê°€ ìë™ìœ¼ë¡œ ìƒˆ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.')

        return {
            'success': True,
            'total_companies': total_companies,
            'integrated_json': str(integrated_json),
            'timestamp': self.timestamp
        }

def main():
    # ê²½ë¡œ ì„¤ì •
    source_dir = Path(r'C:\Users\etlov\agents-workspace\fenomeno_projects\Global_Scouter\Global_Scouter_20251003')
    output_dir = Path(r'C:\Users\etlov\agents-workspace\projects\100xFenok\tools\stock_analyzer\data')

    # í”„ë¡œì„¸ì„œ ì‹¤í–‰
    processor = GlobalScouterDataProcessor(source_dir, output_dir)
    result = processor.run()

    return 0 if result['success'] else 1

if __name__ == '__main__':
    sys.exit(main())
