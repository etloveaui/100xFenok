#!/usr/bin/env python3
"""
Enhanced Data Processor - ì—ì´ì „íŠ¸ë“¤ì„ ìœ„í•œ ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
í”„ë¡œì íŠ¸ ë§¤ë‹ˆì € (Kiro)ê°€ í˜„ì¬ ì‹œìŠ¤í…œ ì•ˆì •í™”ë¥¼ ìœ„í•´ ì‘ì„±
"""

import pandas as pd
import json
import numpy as np
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

class EnhancedDataProcessor:
    """ì—ì´ì „íŠ¸ë“¤ì´ ì‚¬ìš©í•  ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.csv_path = "../../../../fenomeno_projects/Global_Scouter/Global_Scouter_20251003/A_Company.csv"
        self.output_dir = "./data"
        self.current_date = datetime.now()
        
        # 2025ë…„ 10ì›” 8ì¼ ê¸°ì¤€ìœ¼ë¡œ YTD ë¡œì§ ìˆ˜ì •
        self.current_year = 2025
        self.current_month = 10
        
        print(f"ğŸ“… í˜„ì¬ ë‚ ì§œ ì„¤ì •: {self.current_year}ë…„ {self.current_month}ì›”")
        
    def process_weekly_data(self) -> Dict[str, Any]:
        """ë§¤ì£¼ ë°ì´í„° ì²˜ë¦¬ - ì™„ì „ ìë™í™”"""
        print("ğŸš€ Enhanced Data Processor ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. CSV ë¡œë“œ ë° êµ¬ì¡° ë¶„ì„
            raw_df = self._load_and_analyze_csv()
            
            # 2. ì‹¬ì¸µ ë°ì´í„° ì •ì œ
            cleaned_df = self._deep_clean_data(raw_df)
            
            # 3. ìŠ¤ë§ˆíŠ¸ í•„ë“œ ë§¤í•‘
            mapped_df = self._smart_field_mapping(cleaned_df)
            
            # 4. ë°ì´í„° íƒ€ì… ìµœì í™”
            optimized_df = self._optimize_data_types(mapped_df)
            
            # 5. YTD ë¡œì§ ìˆ˜ì • (2025ë…„ 10ì›” ê¸°ì¤€)
            corrected_df = self._fix_ytd_logic(optimized_df)
            
            # 6. í’ˆì§ˆ ê²€ì¦
            validation_result = self._validate_data_quality(corrected_df)
            
            # 7. JSON ì¶œë ¥ ìƒì„±
            output_data = self._generate_enhanced_output(corrected_df)
            
            # 8. íŒŒì¼ ì €ì¥
            output_path = self._save_processed_data(output_data)
            
            # 9. ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_processing_report(raw_df, corrected_df, validation_result)
            
            print("âœ… Enhanced Data Processing ì™„ë£Œ!")
            return {
                'success': True,
                'output_path': output_path,
                'report': report,
                'companies_count': len(corrected_df),
                'indicators_count': len(corrected_df.columns)
            }
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _load_and_analyze_csv(self) -> pd.DataFrame:
        """CSV ë¡œë“œ ë° êµ¬ì¡° ë¶„ì„"""
        print("ğŸ“Š CSV íŒŒì¼ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        df_raw = pd.read_csv(self.csv_path, encoding='utf-8')
        print(f"ì›ë³¸ íŒŒì¼ í¬ê¸°: {df_raw.shape}")
        
        # ì‹¤ì œ í—¤ë”ëŠ” 1ë²ˆì§¸ í–‰
        headers = df_raw.iloc[1].tolist()
        data_rows = df_raw.iloc[2:].copy()
        
        # í—¤ë” ì •ë¦¬ (ì²« ë²ˆì§¸ ë¹ˆ ì»¬ëŸ¼ ì œê±°)
        valid_headers = headers[1:]
        data_rows = data_rows.iloc[:, 1:]
        data_rows.columns = valid_headers[:len(data_rows.columns)]
        
        print(f"ì •ì œëœ ë°ì´í„° í¬ê¸°: {data_rows.shape}")
        return data_rows
    
    def _deep_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì‹¬ì¸µ ë°ì´í„° ì •ì œ - 0-0x2a0x2a íŒ¨í„´ ì™„ì „ ì œê±°"""
        print("ğŸ§¹ ì‹¬ì¸µ ë°ì´í„° ì •ì œ ì¤‘...")
        
        cleaned_df = df.copy()
        
        # 1. 0-0x2a0x2a íŒ¨í„´ ì™„ì „ ì œê±°
        problematic_patterns = [
            '0-0x2a0x2a', '0x2a0x2a', '0-0x2a', 'x2a0x2a',
            'N/A', 'n/a', '#N/A', '#VALUE!', '#DIV/0!'
        ]
        
        for col in cleaned_df.columns:
            try:
                if cleaned_df[col].dtype == 'object':
                    for pattern in problematic_patterns:
                        cleaned_df[col] = cleaned_df[col].astype(str).str.replace(pattern, '', regex=False)
                    
                    # ë¹ˆ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜
                    cleaned_df[col] = cleaned_df[col].replace('', pd.NA)
                    cleaned_df[col] = cleaned_df[col].str.strip()
            except Exception as e:
                print(f"âš ï¸ ì»¬ëŸ¼ {col} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # 2. ë¹ˆ í–‰ ì œê±° (Tickerì™€ Corp ëª¨ë‘ ë¹„ì–´ìˆëŠ” ê²½ìš°)
        important_fields = ['Ticker', 'Corp']
        available_fields = [f for f in important_fields if f in cleaned_df.columns]
        
        if available_fields:
            mask = cleaned_df[available_fields].apply(
                lambda row: row.astype(str).str.strip().ne('').any(), axis=1
            )
            cleaned_df = cleaned_df[mask]
        
        print(f"ì •ì œ ì™„ë£Œ: {len(cleaned_df)} í–‰ (ì œê±°ëœ í–‰: {len(df) - len(cleaned_df)})")
        return cleaned_df
    
    def _smart_field_mapping(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìŠ¤ë§ˆíŠ¸ í•„ë“œ ë§¤í•‘"""
        print("ğŸ¯ ìŠ¤ë§ˆíŠ¸ í•„ë“œ ë§¤í•‘ ì¤‘...")
        
        # í‘œì¤€ í•„ë“œ ë§¤í•‘
        field_mappings = {
            'Corp': 'corpName',
            'WI26': 'industry',  # ì‹¤ì œ ì—…ì¢… í•„ë“œëª… í™•ì¸ í•„ìš”
            'Exchange': 'Exchange',
            'Ticker': 'Ticker'
        }
        
        mapped_df = df.rename(columns=field_mappings)
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ìƒì„±
        required_fields = ['Ticker', 'corpName', 'Exchange', 'industry']
        for field in required_fields:
            if field not in mapped_df.columns:
                # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
                similar_cols = [col for col in mapped_df.columns if field.lower() in col.lower()]
                if similar_cols:
                    mapped_df[field] = mapped_df[similar_cols[0]]
                    print(f"ğŸ“‹ {field} í•„ë“œ ë§¤í•‘: {similar_cols[0]} â†’ {field}")
                else:
                    mapped_df[field] = 'Unknown'
                    print(f"âš ï¸ {field} í•„ë“œ ëˆ„ë½, ê¸°ë³¸ê°’ ì„¤ì •")
        
        return mapped_df
    
    def _optimize_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        print("ğŸ“Š ë°ì´í„° íƒ€ì… ìµœì í™” ì¤‘...")
        
        optimized_df = df.copy()
        
        # ìˆ«ì í•„ë“œ íŒ¨í„´
        numeric_patterns = [
            r'per|pbr|roe|roa|price|return|ratio|rate|growth|margin',
            r'cap|volume|sales|revenue|income|profit|yield',
            r'debt|equity|asset|liability|eps|dps'
        ]
        
        for col in optimized_df.columns:
            if col in ['Ticker', 'corpName', 'Exchange', 'industry']:
                continue  # ë¬¸ì í•„ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                
            col_lower = col.lower()
            is_numeric = any(re.search(pattern, col_lower) for pattern in numeric_patterns)
            
            # ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’ìœ¼ë¡œ ìˆ«ì ì—¬ë¶€ íŒë‹¨
            sample_value = None
            for val in optimized_df[col].dropna().head(5):
                if val is not None:
                    sample_value = str(val)
                    break
            
            if is_numeric or (sample_value and any(char.isdigit() for char in sample_value)):
                # ìˆ«ì ë³€í™˜ ì‹œë„
                optimized_df[col] = pd.to_numeric(optimized_df[col], errors='coerce')
        
        return optimized_df
    
    def _fix_ytd_logic(self, df: pd.DataFrame) -> pd.DataFrame:
        """YTD ë¡œì§ ìˆ˜ì • - 2025ë…„ 10ì›” ê¸°ì¤€"""
        print("ğŸ“… YTD ë¡œì§ ìˆ˜ì • ì¤‘ (2025ë…„ 10ì›” ê¸°ì¤€)...")
        
        corrected_df = df.copy()
        
        # YTD ê´€ë ¨ í•„ë“œ í™•ì¸
        ytd_fields = [col for col in df.columns if 'ytd' in col.lower()]
        
        if ytd_fields:
            print(f"ğŸ” YTD í•„ë“œ ë°œê²¬: {ytd_fields}")
            
            # 2025ë…„ 10ì›”ì´ë¯€ë¡œ YTDëŠ” ì—°ì´ˆë¶€í„° 10ì›”ê¹Œì§€ì˜ ëˆ„ì  ìˆ˜ìµë¥ 
            # ì›ë³¸ YTD ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ë‹¤ê³  ê°€ì •í•˜ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©
            for field in ytd_fields:
                print(f"âœ… {field} í•„ë“œ ìœ ì§€ (2025ë…„ 10ì›” YTD ë°ì´í„°)")
        else:
            print("â„¹ï¸ YTD í•„ë“œ ì—†ìŒ")
        
        # Return (Y) í•„ë“œê°€ ìˆë‹¤ë©´ ì´ê²ƒì´ ì—°ê°„ ìˆ˜ìµë¥ 
        if 'Return (Y)' in corrected_df.columns:
            print("âœ… Return (Y) í•„ë“œ í™•ì¸ - ì—°ê°„ ìˆ˜ìµë¥ ë¡œ ì²˜ë¦¬")
        
        return corrected_df
    
    def _validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        warnings = []
        errors = []
        
        # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['Ticker', 'corpName']
        for field in required_fields:
            if field not in df.columns:
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            elif df[field].isna().sum() > len(df) * 0.1:
                warnings.append(f"í•„ë“œ {field}ì˜ {df[field].isna().sum()}ê°œ ê°’ì´ ëˆ„ë½ë¨")
        
        # 2. ì¤‘ë³µ ë°ì´í„° ê²€ì¦
        if 'Ticker' in df.columns:
            duplicate_tickers = df['Ticker'].duplicated().sum()
            if duplicate_tickers > 0:
                warnings.append(f"ì¤‘ë³µ í‹°ì»¤ {duplicate_tickers}ê°œ ë°œê²¬")
        
        # 3. ìˆ«ì í•„ë“œ ë²”ìœ„ ê²€ì¦
        numeric_validations = {
            'PER (Oct-25)': (0, 1000),
            'PBR (Oct-25)': (0, 100),
            'ROE (Fwd)': (-100, 200)
        }
        
        for field, (min_val, max_val) in numeric_validations.items():
            if field in df.columns:
                out_of_range = ((df[field] < min_val) | (df[field] > max_val)).sum()
                if out_of_range > 0:
                    warnings.append(f"í•„ë“œ {field}ì˜ {out_of_range}ê°œ ê°’ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
        
        return {
            'passed': len(errors) == 0,
            'warnings': warnings,
            'errors': errors,
            'total_records': len(df),
            'valid_records': len(df.dropna(subset=required_fields))
        }
    
    def _generate_enhanced_output(self, df: pd.DataFrame) -> Dict[str, Any]:
        """í–¥ìƒëœ ì¶œë ¥ ë°ì´í„° ìƒì„±"""
        print("ğŸ“¤ í–¥ìƒëœ ì¶œë ¥ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ë°ì´í„° ë³€í™˜
        companies_data = []
        for _, row in df.iterrows():
            company = {}
            for col in df.columns:
                value = row[col]
                if pd.isna(value):
                    company[col] = None
                elif isinstance(value, (int, float)):
                    if np.isnan(value) or np.isinf(value):
                        company[col] = None
                    else:
                        company[col] = float(value)
                else:
                    company[col] = str(value)
            
            # ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¶”ê°€
            ticker = company.get('Ticker', '')
            corp_name = company.get('corpName', '')
            company['searchIndex'] = f"{ticker} {corp_name}".lower()
            
            companies_data.append(company)
        
        return {
            'metadata': {
                'version': '3.0',
                'generated_at': datetime.now().isoformat(),
                'total_companies': len(companies_data),
                'total_fields': len(df.columns),
                'processing_date': f"{self.current_year}ë…„ {self.current_month}ì›”",
                'data_quality_score': self._calculate_quality_score(df),
                'description': 'Enhanced Data Processorë¡œ ì²˜ë¦¬ëœ ê³ í’ˆì§ˆ ë°ì´í„°'
            },
            'companies': companies_data,
            'schema': {
                'fields': [
                    {
                        'name': col,
                        'type': str(df[col].dtype),
                        'null_count': int(df[col].isna().sum()),
                        'unique_count': int(df[col].nunique())
                    }
                    for col in df.columns
                ]
            }
        }
    
    def _calculate_quality_score(self, df: pd.DataFrame) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        total_cells = df.size
        null_cells = df.isna().sum().sum()
        quality_score = ((total_cells - null_cells) / total_cells) * 100
        return round(quality_score, 2)
    
    def _save_processed_data(self, data: Dict[str, Any]) -> str:
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        print("ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
        # ë©”ì¸ ë°ì´í„° íŒŒì¼
        main_output = f"{self.output_dir}/enhanced_summary_data.json"
        with open(main_output, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ë°±ì—… íŒŒì¼
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_output = f"{self.output_dir}/backups/enhanced_data_backup_{timestamp}.json"
        Path(f"{self.output_dir}/backups").mkdir(parents=True, exist_ok=True)
        with open(backup_output, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = Path(main_output).stat().st_size
        print(f"âœ… ë©”ì¸ íŒŒì¼ ì €ì¥: {main_output} ({file_size/1024/1024:.1f}MB)")
        print(f"âœ… ë°±ì—… íŒŒì¼ ì €ì¥: {backup_output}")
        
        return main_output
    
    def _generate_processing_report(self, raw_df: pd.DataFrame, processed_df: pd.DataFrame, validation: Dict) -> Dict[str, Any]:
        """ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            'processing_summary': {
                'original_rows': len(raw_df),
                'processed_rows': len(processed_df),
                'removed_rows': len(raw_df) - len(processed_df),
                'total_columns': len(processed_df.columns),
                'data_quality_score': self._calculate_quality_score(processed_df)
            },
            'validation_results': validation,
            'processing_date': f"{self.current_year}ë…„ {self.current_month}ì›” {datetime.now().day}ì¼",
            'processor_version': '3.0 Enhanced'
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Enhanced Data Processor ì‹œì‘")
    print("í”„ë¡œì íŠ¸ ë§¤ë‹ˆì € (Kiro) - í˜„ì¬ ì‹œìŠ¤í…œ ì•ˆì •í™”")
    print("=" * 60)
    
    processor = EnhancedDataProcessor()
    result = processor.process_weekly_data()
    
    if result['success']:
        print("\nğŸ‰ Enhanced Data Processing ì„±ê³µ!")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   - ê¸°ì—… ìˆ˜: {result['companies_count']:,}ê°œ")
        print(f"   - ì§€í‘œ ìˆ˜: {result['indicators_count']}ê°œ")
        print(f"   - ì¶œë ¥ íŒŒì¼: {result['output_path']}")
        print(f"   - í’ˆì§ˆ ì ìˆ˜: {result['report']['processing_summary']['data_quality_score']}%")
        
        print("\nâœ… ì—ì´ì „íŠ¸ë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì•ˆì •ì ì¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. Claude Code: DataSkeletonì—ì„œ ì´ ë°ì´í„° í™œìš©")
        print("   2. Gemini CLI: WeeklyDataProcessor ê°œë°œ ì‹œ ì°¸ê³ ")
        print("   3. Codex: í†µí•© í…ŒìŠ¤íŠ¸ì—ì„œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    else:
        print(f"\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")

if __name__ == "__main__":
    main()